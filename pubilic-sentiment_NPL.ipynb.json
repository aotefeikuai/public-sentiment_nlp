{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是对于期货数据的处理:这里是用了ricequant上提供的数据,利用historybars()函数获取黄金期货数据并进行价格预测和回测。\n",
    "\n",
    "对于舆情数据：主要使用python爬取东方财富网股吧（http://guba.eastmoney.com/list,huangjin_1.html） 用户发表帖子的标题。对于爬取的数据进行情感分析。\n",
    "\n",
    "对于舆情情感分析。使用Jieba包进行语句分词——去除停用词——计算句子得分。由此流程得到舆情情绪指数的时间序列。\n",
    "\n",
    "对于价格预测：由于舆情数据的情感分析精度不是很高，所以从直觉上感觉直接预测价格的效果可能不够好，所以使用交易日前5天舆情情绪指数进行ols回归得到系数，利用这个系数来判断价格走势。\n",
    "\n",
    "交易策略算法：先计算期货close价格的hurst指数H以做分形判断，\n",
    "大于0.55表明时间序列存在长期记忆性，如果0≤H<0.5，表明粉红噪声(反持续性)即均值回复过程，\n",
    "如果H=0.5，表明时间序列可以用随机游走来描述。\n",
    "接着通过交易日前5天舆情情绪指数的ols回归系数判断价格趋势，系数大于0即认为趋势向上，系数小于0即认为趋势向下。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>close</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>total_turnover</th>\n",
       "      <th>volume</th>\n",
       "      <th>settlement</th>\n",
       "      <th>prev_settlement</th>\n",
       "      <th>open_interest</th>\n",
       "      <th>basis_spread</th>\n",
       "      <th>limit_up</th>\n",
       "      <th>limit_down</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-04-05</th>\n",
       "      <td>280.70</td>\n",
       "      <td>280.70</td>\n",
       "      <td>280.70</td>\n",
       "      <td>280.70</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.70</td>\n",
       "      <td>280.70</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294.70</td>\n",
       "      <td>266.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-06</th>\n",
       "      <td>281.60</td>\n",
       "      <td>280.60</td>\n",
       "      <td>281.60</td>\n",
       "      <td>280.60</td>\n",
       "      <td>1124400.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>281.10</td>\n",
       "      <td>280.70</td>\n",
       "      <td>36.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294.70</td>\n",
       "      <td>266.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-07</th>\n",
       "      <td>283.65</td>\n",
       "      <td>284.15</td>\n",
       "      <td>284.65</td>\n",
       "      <td>283.65</td>\n",
       "      <td>2842300.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>284.20</td>\n",
       "      <td>281.10</td>\n",
       "      <td>38.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>295.15</td>\n",
       "      <td>267.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-10</th>\n",
       "      <td>281.60</td>\n",
       "      <td>281.70</td>\n",
       "      <td>281.70</td>\n",
       "      <td>281.60</td>\n",
       "      <td>3380000.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>281.65</td>\n",
       "      <td>284.20</td>\n",
       "      <td>40.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>298.40</td>\n",
       "      <td>269.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-04-11</th>\n",
       "      <td>281.95</td>\n",
       "      <td>281.40</td>\n",
       "      <td>282.50</td>\n",
       "      <td>281.15</td>\n",
       "      <td>5634100.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>281.70</td>\n",
       "      <td>281.65</td>\n",
       "      <td>44.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>295.70</td>\n",
       "      <td>267.55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              open   close    high     low  total_turnover  volume  \\\n",
       "2017-04-05  280.70  280.70  280.70  280.70             0.0     0.0   \n",
       "2017-04-06  281.60  280.60  281.60  280.60       1124400.0     4.0   \n",
       "2017-04-07  283.65  284.15  284.65  283.65       2842300.0    10.0   \n",
       "2017-04-10  281.60  281.70  281.70  281.60       3380000.0    12.0   \n",
       "2017-04-11  281.95  281.40  282.50  281.15       5634100.0    20.0   \n",
       "\n",
       "            settlement  prev_settlement  open_interest  basis_spread  \\\n",
       "2017-04-05      280.70           280.70           36.0           NaN   \n",
       "2017-04-06      281.10           280.70           36.0           NaN   \n",
       "2017-04-07      284.20           281.10           38.0           NaN   \n",
       "2017-04-10      281.65           284.20           40.0           NaN   \n",
       "2017-04-11      281.70           281.65           44.0           NaN   \n",
       "\n",
       "            limit_up  limit_down  \n",
       "2017-04-05    294.70      266.65  \n",
       "2017-04-06    294.70      266.65  \n",
       "2017-04-07    295.15      267.00  \n",
       "2017-04-10    298.40      269.95  \n",
       "2017-04-11    295.70      267.55  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "au_price=get_price('AU1705', start_date='2017-04-01', end_date='2017-05-01', frequency='1d', fields=None, adjust_type='pre', skip_suspended=False, country='cn')\n",
    "au_price[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 需用到的函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import re\n",
    "import jieba\n",
    "import codecs\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "#使用jieba 函数  对 sentence 文本进行分词\n",
    "def sent2word(sentence):\n",
    "#调用jieba进行分词\n",
    "    segList = jieba.cut(sentence)\n",
    "\n",
    "#分词后的结果存为segResult 为list类型\n",
    "    segResult = []\n",
    "    for w in segList:\n",
    "        segResult.append(w)\n",
    "\n",
    "#调用 readLines 读取停用词\n",
    "    stopwords = readLines('stop_words.txt')\n",
    "\n",
    "#如果是停用词 就不保存到newSent\n",
    "    newSent = []\n",
    "    for word in segResult:\n",
    "        if word+'\\n' in stopwords:\n",
    "            continue\n",
    "        else:\n",
    "            newSent.append(word)\n",
    "#返回newSent\n",
    "    return newSent\n",
    "\n",
    "#直接对 sentence 进行分词  不使用停用词 并返回（主要是根据word需要这个操作）\n",
    "def returnsegResult(sentence):\n",
    "\n",
    "    segResult = []\n",
    "    segList = jieba.cut(sentence)\n",
    "\n",
    "    for w in segList:\n",
    "        segResult.append(w)\n",
    "    return segResult\n",
    "\n",
    "#获取 filepath 目录下的所有文件目录并返回\n",
    "def eachFile(filepath):\n",
    "    pathDir =  os.listdir(filepath)\n",
    "    child=[]\n",
    "    for allDir in pathDir:\n",
    "        child.append(os.path.join('%s/%s' % (filepath, allDir)))\n",
    "    return child\n",
    "\n",
    "#读取 filename路径 的每一行数据 并返回 转换为GBK\n",
    "def readLines(filename):\n",
    "    fopen = open(filename, 'r')\n",
    "    data=[]\n",
    "    for x in fopen.readlines():\n",
    "        if x.strip() != '':\n",
    "                data.append(x.strip())\n",
    "\n",
    "    fopen.close()\n",
    "    return data\n",
    "\n",
    "#读取 filename路径 的每一行数据 并返回\n",
    "def readLines2(filename):\n",
    "    fopen = open(filename, 'r')\n",
    "    data=[]\n",
    "    for x in fopen.readlines():\n",
    "        if x.strip() != '':\n",
    "                data.append(x.strip())\n",
    "\n",
    "    fopen.close()\n",
    "    return data\n",
    "\n",
    "#主要为情感定位 \n",
    "def words():\n",
    "    #情感词\n",
    "    senList = readLines2('BosonNLP_sentiment_score.txt')\n",
    "    senDict = defaultdict()\n",
    "    for s in senList:\n",
    "        senDict[s.split(' ')[0]] = s.split(' ')[1]\n",
    "    #否定词\n",
    "    notList = readLines2('notDict.txt')\n",
    "    #程度副词\n",
    "    degreeList = readLines2('degreeDict.txt')\n",
    "    degreeDict = defaultdict()\n",
    "    for d in degreeList:\n",
    "#         print(d.split(',')[0])\n",
    "        degreeDict[d.split(',')[0]] = d.split(',')[1]\n",
    "\n",
    "    return senDict,notList,degreeDict\n",
    "\n",
    "#  根据情感定位  获得句子相关得分\n",
    "def classifyWords(wordDict,senDict,notList,degreeDict):\n",
    "\n",
    "    senWord = defaultdict()\n",
    "    notWord = defaultdict()\n",
    "    degreeWord = defaultdict()\n",
    "    for word in wordDict.keys():\n",
    "        if word in senDict.keys() and word not in notList and word not in degreeDict.keys():\n",
    "            senWord[wordDict[word]] = senDict[word]\n",
    "        elif word in notList and word not in degreeDict.keys():\n",
    "            notWord[wordDict[word]] = -1\n",
    "        elif word in degreeDict.keys():\n",
    "            degreeWord[wordDict[word]] = degreeDict[word]\n",
    "    return senWord, notWord, degreeWord\n",
    "\n",
    "#计算句子得分 \n",
    "def scoreSent(senWord, notWord, degreeWord, segResult):\n",
    "    W = 1\n",
    "    score = 0\n",
    "    # 存所有情感词的位置的列表\n",
    "    senLoc = list(senWord.keys())\n",
    "#     print(senLoc[0])\n",
    "    notLoc = notWord.keys()\n",
    "    degreeLoc = degreeWord.keys()\n",
    "    senloc = -1\n",
    "    # notloc = -1\n",
    "    # degreeloc = -1\n",
    "    # 遍历句中所有单词segResult，i为单词绝对位置\n",
    "    for i in range(0, len(segResult)):\n",
    "        # 如果该词为情感词\n",
    "        if i in senLoc:\n",
    "            # loc为情感词位置列表的序号\n",
    "            senloc += 1\n",
    "            # 直接添加该情感词分数\n",
    "            score += W * float(senWord[i])\n",
    "            # print \"score = %f\" % score\n",
    "            if senloc < len(senLoc) - 1:\n",
    "                # 判断该情感词与下一情感词之间是否有否定词或程度副词\n",
    "                # j为绝对位置\n",
    "                for j in range(senLoc[senloc], senLoc[senloc + 1]):\n",
    "                    # 如果有否定词\n",
    "                    if j in notLoc:\n",
    "                        W *= -1\n",
    "                    # 如果有程度副词\n",
    "                    elif j in degreeLoc:\n",
    "                        W *= float(degreeWord[j])\n",
    "        # i定位至下一个情感词\n",
    "        if senloc < len(senLoc) - 1:\n",
    "            i = senLoc[senloc + 1]\n",
    "    return score\n",
    "\n",
    "#列表 转 字典\n",
    "def listToDist(wordlist):\n",
    "    data={}\n",
    "    for x in range(0, len(wordlist)):\n",
    "        data[wordlist[x]]=x\n",
    "    return data\n",
    "\n",
    "#绘图相关 \n",
    "def runplt():\n",
    "    plt.figure()\n",
    "    plt.title('test')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    #这里定义了  图的长度 比如 2000条数据 就要 写 0,2000  \n",
    "    plt.axis([0,1000,-10,10])\n",
    "    plt.grid(True)\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下为爬取舆情数据和舆情信息处理为舆情情绪指数过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page:1\n",
      "page:2\n",
      "page:3\n",
      "page:4\n",
      "page:5\n",
      "page:6\n",
      "page:7\n",
      "page:8\n",
      "page:9\n",
      "page:10\n",
      "page:11\n",
      "page:12\n",
      "page:13\n",
      "page:14\n",
      "page:15\n",
      "page:16\n",
      "page:17\n",
      "page:18\n",
      "page:19\n",
      "page:20\n",
      "page:21\n",
      "page:22\n",
      "page:23\n",
      "page:24\n",
      "page:25\n",
      "page:26\n",
      "page:27\n",
      "page:28\n",
      "page:29\n",
      "page:30\n",
      "page:31\n",
      "page:32\n",
      "page:33\n",
      "page:34\n",
      "page:35\n",
      "page:36\n",
      "page:37\n",
      "page:38\n",
      "page:39\n",
      "page:40\n",
      "page:41\n",
      "page:42\n",
      "page:43\n",
      "page:44\n",
      "page:45\n",
      "page:46\n",
      "page:47\n",
      "page:48\n",
      "page:49\n",
      "page:50\n",
      "page:51\n",
      "page:52\n",
      "page:53\n",
      "page:54\n",
      "page:55\n",
      "page:56\n",
      "page:57\n",
      "page:58\n",
      "page:59\n",
      "page:60\n",
      "page:61\n",
      "page:62\n",
      "page:63\n",
      "page:64\n",
      "page:65\n",
      "page:66\n",
      "page:67\n",
      "page:68\n",
      "page:69\n",
      "page:70\n",
      "page:71\n",
      "page:72\n",
      "page:73\n",
      "page:74\n",
      "page:75\n",
      "page:76\n",
      "page:77\n",
      "page:78\n",
      "page:79\n",
      "page:80\n",
      "page:81\n",
      "page:82\n",
      "page:83\n",
      "page:84\n",
      "page:85\n",
      "page:86\n",
      "page:87\n",
      "page:88\n",
      "page:89\n",
      "page:90\n",
      "page:91\n",
      "page:92\n",
      "page:93\n",
      "page:94\n",
      "page:95\n",
      "page:96\n",
      "page:97\n",
      "page:98\n",
      "page:99\n",
      "page:100\n",
      "page:101\n",
      "page:102\n",
      "page:103\n",
      "page:104\n",
      "page:105\n",
      "page:106\n",
      "page:107\n",
      "page:108\n",
      "page:109\n",
      "page:110\n",
      "page:111\n",
      "page:112\n",
      "page:113\n",
      "page:114\n",
      "page:115\n",
      "page:116\n",
      "page:117\n",
      "page:118\n",
      "page:119\n",
      "page:120\n",
      "page:121\n",
      "page:122\n",
      "page:123\n",
      "page:124\n",
      "page:125\n",
      "page:126\n",
      "page:127\n",
      "page:128\n",
      "page:129\n",
      "page:130\n",
      "page:131\n",
      "page:132\n",
      "page:133\n",
      "page:134\n",
      "page:135\n",
      "page:136\n",
      "page:137\n",
      "page:138\n",
      "page:139\n",
      "page:140\n",
      "page:141\n",
      "page:142\n",
      "page:143\n",
      "page:144\n",
      "page:145\n",
      "page:146\n",
      "page:147\n",
      "page:148\n",
      "page:149\n",
      "page:150\n",
      "page:151\n",
      "page:152\n",
      "page:153\n",
      "page:154\n",
      "page:155\n",
      "page:156\n",
      "page:157\n",
      "page:158\n",
      "page:159\n",
      "page:160\n",
      "page:161\n",
      "page:162\n",
      "page:163\n",
      "page:164\n",
      "page:165\n",
      "page:166\n",
      "page:167\n",
      "page:168\n",
      "page:169\n",
      "page:170\n",
      "page:171\n",
      "page:172\n",
      "page:173\n",
      "page:174\n",
      "page:175\n",
      "page:176\n",
      "page:177\n",
      "page:178\n",
      "page:179\n",
      "page:180\n",
      "page:181\n",
      "page:182\n",
      "page:183\n",
      "page:184\n",
      "page:185\n",
      "page:186\n",
      "page:187\n",
      "page:188\n",
      "page:189\n",
      "page:190\n",
      "page:191\n",
      "page:192\n",
      "page:193\n",
      "page:194\n",
      "page:195\n",
      "page:196\n",
      "page:197\n",
      "page:198\n",
      "page:199\n",
      "                              title read\n",
      "05-11                         白银已反弹   44\n",
      "05-11      钟鼎策金5.11英国央行利率料按兵不动，英镑或面  114\n",
      "05-11       叶世诚:5.11黄金原油天然气今日走势最新消息  275\n",
      "05-11      阮舒窈：5.11纽美破位刷下限 欧元兑美元行情分  183\n",
      "05-11      阮舒窈：5.11新西兰联储放鸽 纽元兑美元操作建  347\n",
      "05-11         李淼盛：原油空头趋势逆转，减产库存双重利好  200\n",
      "05-11            黄逸尘08：现货黄金日内短线交易技巧  353\n",
      "05-11  黎明为金：5.11 OPEC减产延期 原油震荡看48美元  299\n",
      "05-11      余亦航：黄金市场受美联储掌控，5.11黄金原油行  217\n",
      "05-11         王泽至：金价1225强力压制，探底还未结束  538\n",
      "           emotion\n",
      "02-10  1084.943105\n",
      "05-11  1157.880457\n",
      "05-07   209.694886\n",
      "05-10  1422.278356\n",
      "05-06   273.515068\n",
      "05-09  1486.098539\n",
      "05-08  1440.512694\n",
      "05-03  1312.872329\n",
      "05-05  1321.989498\n",
      "05-04  1166.997626\n",
      "04-29   118.523196\n",
      "05-02  1012.005753\n",
      "05-01   446.741279\n",
      "04-27   911.716895\n",
      "04-30   136.757534\n",
      "04-26   884.365388\n",
      "04-25   811.428037\n",
      "04-28   820.545205\n",
      "04-24   811.428037\n",
      "04-19  2188.120548\n",
      "04-20  1121.411781\n",
      "04-23   145.874703\n",
      "04-22   255.280731\n",
      "04-18  2443.401279\n",
      "04-21  1002.888584\n",
      "04-17  2488.987123\n",
      "04-14  1431.395525\n",
      "04-13  2908.376895\n",
      "04-16   200.577717\n",
      "04-12  2561.924475\n",
      "...            ...\n",
      "03-11   565.264475\n",
      "03-10  2416.049772\n",
      "03-09  2033.128676\n",
      "03-12   209.694886\n",
      "03-08  1969.308493\n",
      "03-07  2169.886210\n",
      "03-06  2024.011507\n",
      "03-05   164.109041\n",
      "03-03  2352.229589\n",
      "03-02  2188.120548\n",
      "03-01  1969.308493\n",
      "03-04   556.147306\n",
      "02-28  1850.785297\n",
      "02-27  1859.902466\n",
      "02-26   264.397900\n",
      "02-25   373.803927\n",
      "02-24  1413.161187\n",
      "02-23  1467.864201\n",
      "02-21   957.302740\n",
      "02-20   811.428037\n",
      "02-22  1303.755160\n",
      "02-19   100.288858\n",
      "02-18    63.820183\n",
      "02-17   747.607854\n",
      "02-16   929.951233\n",
      "02-15   784.076530\n",
      "02-14   556.147306\n",
      "02-13   556.147306\n",
      "02-12    82.054521\n",
      "02-11    45.585845\n",
      "\n",
      "[91 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import lxml\n",
    "import re\n",
    "import urllib\n",
    "import sys\n",
    "import xlrd\n",
    "import pandas as pd\n",
    "row_list = []\n",
    "df_sentiment=pd.DataFrame()\n",
    "# 获取各行数据\n",
    "for page in range(1,200):\n",
    "    address=\"http://guba.eastmoney.com/list,\"+'huangjin'+\",f_\"+str(page)+\".html\"\n",
    "    print('page:'+str(page))\n",
    "    def getHtml(url):   #获取页面内容\n",
    "        req_header = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "            'Accept': 'text/html;q=0.9,*/*;q=0.8',\n",
    "            'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "            'Accept-Encoding': 'gzip',\n",
    "            'Connection': 'close',\n",
    "            'Referer': None\n",
    "            }\n",
    "        req_timeout = 5\n",
    "        req = requests.get(url,headers=req_header)\n",
    "        # resp = requests.urlopen(req, None, req_timeout)\n",
    "        html=req.content\n",
    "        return html\n",
    "    html = getHtml(address)\n",
    "    soup=BeautifulSoup(html,\"lxml\") #利用bs来解析页面\n",
    "    article=soup.find_all(class_='articleh')\n",
    "\n",
    "    for imga in article:\n",
    "        read = imga.find_all(class_='l1')   #通过beautifulsoup找到需要的标题作者等等内容\n",
    "        read=read[0].string\n",
    "        title= imga.find_all(class_='l3')\n",
    "        title= title[0].string\n",
    "        author = imga.find_all(class_='l4')\n",
    "        author = author[0].string\n",
    "        url = imga.find_all(class_='l3')\n",
    "        pat = re.compile(r'href=\"([^\"]*)\"') #通过正则找取url 如换网站需更改此正则表达式\n",
    "        h = pat.search(str(url[0]))\n",
    "        url = h.group(1)\n",
    "        url='http://guba.eastmoney.com'+url\n",
    "        comment = imga.find_all(class_='l2')\n",
    "        comment = comment[0].string\n",
    "        date=imga.find_all(class_='l6')\n",
    "        date = date[0].string\n",
    "        if title is not None:\n",
    "            df1=pd.DataFrame([[title,read]],[date],['title','read'])\n",
    "            df_sentiment=df_sentiment.append(df1)\n",
    "            element=sent2word(data)\n",
    "            words_value=words()\n",
    "            step1=listToDist(element)\n",
    "            step=classifyWords(step1,words_value[0],words_value[1],words_value[2])\n",
    "            step2=float(scoreSent(step[0],step[1],step[2],returnsegResult(data)))\n",
    "            df1=pd.DataFrame(float(np.array([step2])),[date],['emotion'])\n",
    "            if len(df.index)== 0:\n",
    "                df=df.append(df1)\n",
    "            if date in (df.index):\n",
    "                df.loc[[date]]=(df.loc[[date]]) +step2\n",
    "            else:\n",
    "                df=df.append(df1)\n",
    "        # print read\n",
    "        # print comment\n",
    "        # print date\n",
    "        # print url\n",
    "        # print author\n",
    "print(df_sentiment[0:10])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_sentiment.to_csv('sentiment.csv')\n",
    "df.to_csv('emotion.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
